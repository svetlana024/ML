{"cells":[{"metadata":{},"cell_type":"markdown","source":"# AI Lab: Генерация текстов/последовательностей с помощью нейронных сетей\n\n\nВыполнила: Власова Светлана\n\nГруппа: М8O-306Б\n\nВараинт: 3","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Импорт необходимых библиотек.**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Embedding, GRU, LSTM, BatchNormalization\nfrom tensorflow.keras.layers import Dropout, Dense, SimpleRNN\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\nimport numpy as np\nimport os\nimport time\n\nimport re\n\nimport nltk\n#nltk.download('all')\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\n#from jupyterthemes import jtplot\n#jupyterthemes.download(jtplot)\nmatplotlib.rcParams['figure.figsize'] = (15,10)\n#jtplot.style('onedork')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Создание датасета","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Работа с текстом.**\n\nДля формирования датасета я воспользовалась сайтом https://www.gutenberg.org/.\nМой выбор пал на произведение Ф. М. Достоевского \"Идиот\".\nИмпорт текста представлен ниже.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"book = 'idiot.txt'\npath = 'https://www.gutenberg.org/files/2638/2638-0.txt'\ntext = open(tf.keras.utils.get_file(book, path), 'rb').read().decode(encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Взглянем на начало текста.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(text[:250])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Функционал для \"чистки\" текста.*\n\nДля того, чтобы не засорять набор данных, я решила отчистить текст от ненужной информации, которая есть перед первой и последней главами - вроде биографической справки или примечаний переводчиков. Также, я посчитала нужным извлечь из текста вставки, относящиеся к иллюстрациям. Это все реализовано в виде функции *clear_text(text)*, которая представлена ниже.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def clear_text(text):\n    text = text[text.find('PART I'):text.find('End of the Project Gutenberg')]\n    ' '.join(text.split('\\r\\n'))\n    return ''.join(text.split('[Illustration]'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Разбиение текста по главам представлено ниже - это позволит избавить от вставок 'PART #номер_главы'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_by_parts(text):\n    parts = \"\"\n    text = text[text.find('\\n'):]\n    while text[:text.find('PART ')]!= '':\n        parts +=  text[:text.find('PART ')]\n        text = text[text.find('PART '):]\n        text = text[text.find('\\n'):]\n    return parts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = split_by_parts(clear_text(text))\nprint(text[:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Длина текста: ', len(text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрим, сколько уникальных символов есть в нашем тексте. Создадим словарь.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = sorted(set(text))\nvocab_size = len(vocab)\nprint ('Текст содержит ', vocab_size, ' уникальных символа.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Итак, мы приблизили текст к удобному для дальнейшего обучения виду, избавившись от \"выбросов\" в виде технической информации или ненужных вставок, однако, это еще не все.\n\nТеперь **векторизуем** *текст* - так, чтобы любая последовательность могла отображаться в числовую последовательность. Для этого создадим две таблицы поиска - одна будет отображать символы в числа, а другая числа в символы.\n\nТак, каждый уникальный символ будет иметь числовое отображение на полуинтервале $\\left[0, len(vocab)\\right).$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"char_to_id = {c:i for i, c in enumerate(vocab)} #таблица поиска по символу его числового значения\nid_to_char = np.array(vocab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Векторизуем наш текст.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorized_text = np.array([char_to_id[c] for c in text])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Полученная таблица: ')\nfor c, count in zip(char_to_id, range(20)):\n    print('  {:4s}: {:3d},'.format(repr(c), char_to_id[c]))\nprint('  ...\\n')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Проиллюстрируем \"процесс\" векторизации текста.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('{} ---- отображение char в int ---- > {}'.format(repr(text[:15]), vectorized_text[:15]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Формирование датасета.**\n\nПриведем последовательность чисел в тензорный вид с помощью функции *tf.data.Dataset.from_tensor_slices().*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices(vectorized_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Далее, будем разбивать датасет на последовательности символов фиксированной длины **SEQ_LEN.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SEQ_LEN = 150","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Т.к. элементом последовательности является один символ, будем разбивать набор данных на батчи размером **SEQ_LEN + 1**, так, чтобы входная и целевая последовательности были одинаковой длины - **SEQ_LEN**, и целевая последовательность получалась из входной сдвигом на 1 символ вправо, отличалась от нее одним символом в рамках выделенного батча.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Воспользуемся *batch-*методом из *tf.data.Dataset* для формирования последовательностей нужной длины.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences = dataset.batch(SEQ_LEN + 1, drop_remainder=True)\nprint('Число последовательностей длины ', SEQ_LEN + 1, ': ', len(list(sequences.as_numpy_iterator())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Взглянем на первые три последовательности в исходном виде.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in sequences.take(5):\n  print(repr(''.join(id_to_char[i.numpy()])), end=\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Итак, мы имеем набор из последовательностей размером **SEQ_LEN + 1** чисел. Для обучения следует разбить этот датасет на 2 колонки:\n* обучающую последовательность, состоящую из перых *SEQ_LEN* тензоров.\n* тестовую последовательность, состоящую из последних *SEQ_LEN* тензоров.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Для ускорения обучения перемешаем последовательности в датасете и разобъем их на наборы по **BATCH_SIZE** элементов.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BUFFER_SIZE = 10000\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\ndataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Датасет готов. Теперь перейдем к следующему шагу - проектированию моделей.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Проектирование моделей","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Полносвязная RNN-сеть**\n\nСоздадим директорию, где будут храниться[](http://) данные полносвязной RNN.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rnn_dir = \"./full_rnn\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! mkdir -p full_rnn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Полносвязная RNN-сеть будет состоять из следующих слоев:\n1. Embedding - входной слой. Обучаемая справочная таблица, которая отображает номера каждого символа в вектор с размерами embedding_dim;\n2. SimpleRNN - полносвязный рекуррентный слой с функцией инициализации Ксавьера;\n3. Dense - выходной слой сети с выходной размерностью *vocab_size*.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_rnn(vocab_size, embedding_dim, batch_size, rnn_units):\n    model = Sequential([\n      # 1 layer\n        Embedding(vocab_size, embedding_dim,\n                  batch_input_shape=[batch_size, None]),\n      # 2 layer \n        SimpleRNN(rnn_units, return_sequences=True, stateful=False, \n          recurrent_initializer='glorot_uniform'), \n      # 3 layer\n        Dense(vocab_size, kernel_initializer=\"glorot_uniform\")\n    ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Однослойная LSTM-сеть**\n\nСоздадим директорию, где будут храниться данные однослойной LSTM.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sing_dir = \"./single_lstm\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! mkdir -p single_lstm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Однослойная LSTM-сеть будет состоять из следующих слоев:\n\n1. Embedding - входной слой. Обучаемая справочная таблица, которая отображает номера каждого символа в вектор с размерами embedding_dim;\n2. LSTM - слой с функцией инициализации Ксавьера;\n3. Dense - выходной слой сети с выходной размерностью vocab_size.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_single_lstm(vocab_size, embedding_dim, batch_size, rnn_units):\n    model = Sequential([\n      # 1 layer\n        Embedding(vocab_size, embedding_dim, \n                  batch_input_shape=[batch_size, None]),\n      # 2 layer \n        LSTM(rnn_units, return_sequences=True, stateful=False, \n          recurrent_initializer='glorot_uniform'), \n      # 3 layer\n        Dense(vocab_size, kernel_initializer=\"glorot_uniform\")\n    ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Двухслойная LSTM-сеть**\n\nСоздадим директорию, где будут храниться данные двухслойной LSTM.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"doub_dir = \"./double_lstm\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! mkdir -p double_lstm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Двухслойная LSTM-сеть будет состоять из следующих слоев:\n\n1. Embedding - входной слой. Обучаемая справочная таблица, которая отображает номера каждого символа в вектор с размерами embedding_dim;\n2. LSTM - слой с функцией инициализации Ксавьера;\n3. LSTM - слой с функцией инициализации Ксавьера;\n4. Dense - выходной слой сети с выходной размерностью vocab_size.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_double_lstm(vocab_size, embedding_dim, batch_size, rnn_units):\n    model = Sequential([\n      # 1 layer\n        Embedding(vocab_size, embedding_dim, \n                  batch_input_shape=[batch_size, None]),\n      # 2 layer \n        LSTM(rnn_units, return_sequences=True, stateful=False, \n          recurrent_initializer='glorot_uniform'),\n      # 3 layer \n        LSTM(rnn_units, return_sequences=True, stateful=False, \n          recurrent_initializer='glorot_uniform'),\n      # 4 layer\n        Dense(vocab_size, kernel_initializer=\"glorot_uniform\")\n    ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Однослойная GRU-сеть**\n\nСоздадим директорию, где будут храниться данные однослойной GRU.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gru_dir = \"./single_gru\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! mkdir -p single_gru","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Однослойная GRU-сеть будет состоять из следующих слоев:\n\n1. Embedding - входной слой. Обучаемая справочная таблица, которая отображает номера каждого символа в вектор с размерами embedding_dim;\n2. GRU - слой с функцией инициализации Ксавьера;\n3. Dense - выходной слой сети с выходной размерностью vocab_size.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_gru(vocab_size, embedding_dim, batch_size, rnn_units):\n    model = Sequential([\n      # 1 layer\n        Embedding(vocab_size, embedding_dim, \n                  batch_input_shape=[batch_size, None]),\n      # 2 layer \n        GRU(rnn_units, return_sequences=True, stateful=False, \n          recurrent_initializer='glorot_uniform'), \n      # 3 layer\n        Dense(vocab_size, kernel_initializer=\"glorot_uniform\")\n    ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Обучение моделей\n\nПри обучении можно сохранять параметры обученной модели или же саму модель. В процессе обучения будем сохранять веса, если после очередной эпохи модель показывает лучший результат - минимальное значение ошибки.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def checkpoint_creator(checkpoint_dir = \"./\"):\n    checkpoint_prefix = os.path.join(checkpoint_dir, \"checkpoints/ckpt_model\")\n    checkpoint_callback = ModelCheckpoint(filepath=checkpoint_prefix,\n                                          monitor=\"loss\", \n                                          mode=\"min\",\n                                          save_best_only=True,\n                                          save_weights_only=True)\n    return checkpoint_callback","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Функция потерь**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Параметры моделей**\n\nЗададим необходимые параметры моделей. \n\nК слову, такие параметры, как длина последовтаельности **SEQ_LEN** или размер батча **BATCH_SIZE,** уже учлись при формировании датасета.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"EMB_DIM = 256\nUNITS = 512\nEPOCHS = 50","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Сборка моделей**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Полносвязная RNN-сеть.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full_rnn = build_rnn(vocab_size, EMB_DIM, BATCH_SIZE, UNITS)\nfull_rnn.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Однослойная LSTM-сеть.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"single_lstm = build_single_lstm(vocab_size, EMB_DIM, BATCH_SIZE, UNITS)\nsingle_lstm.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Двухслойная LSTM-сеть.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"double_lstm = build_double_lstm(vocab_size, EMB_DIM, BATCH_SIZE, UNITS)\ndouble_lstm.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Однослойная GRU-сеть.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"single_gru = build_gru(vocab_size, EMB_DIM, BATCH_SIZE, UNITS)\nsingle_gru.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Скомпилируем, используя стандартный оптимизатор Адама. В качестве функции потерь используем описанную выше кроссэнтропию.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full_rnn.compile(optimizer='adam', loss=loss)\nsingle_lstm.compile(optimizer='adam', loss=loss)\ndouble_lstm.compile(optimizer='adam', loss=loss)\nsingle_gru.compile(optimizer='adam', loss=loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Обучение.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Обучим полносвязную *RNN* модель.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_callbacks = [EarlyStopping(monitor=\"loss\", patience=3), checkpoint_creator(rnn_dir)]\nrnn_history = full_rnn.fit(dataset, epochs=EPOCHS, callbacks=my_callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Обучим однослойную *LSTM* модель.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_callbacks = [EarlyStopping(monitor=\"loss\", patience=3), checkpoint_creator(sing_dir)]\nlstm_single_history = single_lstm.fit(dataset, epochs=EPOCHS, callbacks=my_callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Обучим двухслойную *LSTM* модель.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_callbacks = [EarlyStopping(monitor=\"loss\", patience=3), checkpoint_creator(doub_dir)]\nlstm_double_history = double_lstm.fit(dataset, epochs=EPOCHS, callbacks=my_callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Обучим однослойную *GRU* модель.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_callbacks = [EarlyStopping(monitor=\"loss\", patience=3), checkpoint_creator(gru_dir)]\ngru_single_history = single_gru.fit(dataset, epochs=EPOCHS, callbacks=my_callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Визуализируем процессы обучения всех четырех моделей с помощью графика зависимости величины *loss* от количества эпох.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(rnn_history.history['loss'])\nplt.plot(lstm_single_history.history['loss'])\nplt.plot(lstm_double_history.history['loss'])\nplt.plot(gru_single_history.history['loss'], color = 'green')\nplt.title('Процесс обучения')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(['Полносвязная RNN', 'Однослойная LSTM', 'Двухслойная LSTM', \n            'Однослойная GRU'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Тестирование моделей\n\n\nМы обучили наши модели на батчах размера **BATCH_SIZE**, однако при генерации текста мы отправляем в модель одну последовательность произвольного размера. Перестроим модели для еденичного размера батча и загрузим в них оптимальные веса обученных моделей из контрольных точек.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = os.path.join(rnn_dir, \"checkpoints/\")\n\nfull_rnn = build_rnn(vocab_size, EMB_DIM, 1, UNITS)\nfull_rnn.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nfull_rnn.build(tf.TensorShape([1, None]))\nfull_rnn.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = os.path.join(sing_dir, \"checkpoints/\")\n\nsingle_lstm = build_single_lstm(vocab_size, EMB_DIM, 1, UNITS)\nsingle_lstm.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nsingle_lstm.build(tf.TensorShape([1, None]))\nsingle_lstm.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = os.path.join(doub_dir, \"checkpoints/\")\n\ndouble_lstm = build_double_lstm(vocab_size, EMB_DIM, 1, UNITS)\ndouble_lstm.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\ndouble_lstm.build(tf.TensorShape([1, None]))\ndouble_lstm.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = os.path.join(gru_dir, \"checkpoints/\")\n\nsingle_gru = build_single_gru(vocab_size, EMB_DIM, 1, UNITS)\nsingle_gru.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nsingle_gru.build(tf.TensorShape([1, None]))\nsingle_gru.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Сохранение моделей**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = os.path.join(rnn_dir, \"full_rnn.h5\")\nfull_rnn.save(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = os.path.join(sing_dir, \"single_lstm.h5\")\nsingle_lstm.save(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = os.path.join(doub_dir, \"double_lstm.h5\")\ndouble_lstm.save(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = os.path.join(gru_dir, \"gru_dir.h5\")\nsingle_gru.save(filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Генерация текста**\n\nФункция генерации текста работает следующим образом:\n\n* начальная строка векторизуется, инициализируется состояние обученой модели и устанавливается длина сгенерированной выходной последовательности;\n\n* прогнозируется распределение следующего символа, исходя из входной строки и состояния модели;\n\n* предсказанный символ становится следующим символом, который подается на вход модели;\n\n* состояние, возвращаемое моделью, передается обратно в модель, так что теперь у нее больше контекста, а не только один символ. После каждой эпохи контекст модели расширяется.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_text(model, start_string, num_generate=100):\n\n  # Converting our start string to numbers (vectorizing)\n  input_eval = [char_to_id[c] for c in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = []\n\n  # Low temperatures results in more predictable text.\n  # Higher temperatures results in more surprising text.\n  # Experiment to find the best setting.\n  temperature = 1.0\n\n  # Here batch size == 1\n  model.reset_states()\n  for i in range(num_generate):\n    predictions = model(input_eval)\n    # remove the batch dimension\n    predictions = tf.squeeze(predictions, 0)\n\n    # using a categorical distribution to predict the character returned by the model\n    predictions = predictions / temperature\n    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n    # We pass the predicted character as the next input to the model\n    # along with the previous hidden state\n    input_eval = tf.expand_dims([predicted_id], 0)\n\n    text_generated.append(id_to_char[predicted_id])\n\n  return (start_string + ''.join(text_generated))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_text(full_rnn, \"The result of the training is\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_text(single_lstm, \"The result of the training is\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_text(double_lstm, \"The result of the training is\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_text(single_gru, \"The result of the training is\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
