{"cells":[{"metadata":{},"cell_type":"markdown","source":"Выполнила: Власова Светлана, группа 306."}
{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Подготовка датасета"},{"metadata":{},"cell_type":"markdown","source":"Подготовим набор данных. Я это делала в предыдущей лабораторной работе в более подробной форме. Сейчас же, чтобы освежить в памяти \"содержание\" набора данных, я решила выполнить еще раз основные манипуляции с исходным датасетом, найденным на Kaggle.com.\nДля задачи бинарной классификации возьмем второй выбранный датасет, где в качестве целевого признака выступила валентность трека, условно характеризуемая двумя классами - \"грустный\" и \"веселый\". Изначально этот признак принимал значения на отрезке [0, 1.0], но был преобразован в бинарный."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Загрузка датасета."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"spot_tracks_file_path = '../input/ultimate-spotify-tracks-db/SpotifyFeatures.csv'\ndata = pd.read_csv(spot_tracks_file_path)","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Преобразование категориальных признаков в численные."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['key'] == 'C', 'key'] = 0\ndata.loc[data['key'] == 'C#', 'key'] = 1\ndata.loc[data['key'] == 'D', 'key'] = 2\ndata.loc[data['key'] == 'D#', 'key'] = 3\ndata.loc[data['key'] == 'E', 'key'] = 4\ndata.loc[data['key'] == 'F', 'key'] = 5\ndata.loc[data['key'] == 'F#', 'key'] = 6\ndata.loc[data['key'] == 'G', 'key'] = 7\ndata.loc[data['key'] == 'G#', 'key'] = 8\ndata.loc[data['key'] == 'A', 'key'] = 9\ndata.loc[data['key'] == 'A#', 'key'] = 10\ndata.loc[data['key'] == 'B', 'key'] = 11","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['mode'] == 'Major', 'mode'] = 1\ndata.loc[data['mode'] == 'Minor', 'mode'] = 0","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['time_signature'].unique()\nstring = []\nfor i in data['time_signature'].unique():\n    for j in i.split(sep='/'):\n        string.append(int(j))\n    data.loc[data['time_signature'] == i, 'time_signature'] = string[0]/string[1]\n    string.clear()\ndata['time_signature'] =  pd.to_numeric(data.time_signature, errors='coerce')","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del data['track_id']","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Преобразование целевого признака для бинарной классификации."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['valence'] < 0.5, 'valence'] = 0\ndata.loc[data['valence'] >= 0.5, 'valence'] = 1","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def code_mean(data, cat_feature, real_feature):\n    return(data[cat_feature].map(data.groupby(cat_feature)[real_feature].mean()))","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Генерация нового признака на основе двух других."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['artist_name_mean_energy'] = code_mean(data, 'artist_name', 'energy')","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Определение обучающих признаков."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['popularity','danceability', 'duration_ms', 'energy', 'loudness', 'speechiness', 'instrumentalness', 'tempo', 'mode', 'time_signature','artist_name_mean_energy']\nX = data[features]\ny = data['valence'].to_numpy()","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Получившаяся выборка представлена ниже."},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"          popularity   danceability   duration_ms         energy  \\\ncount  232725.000000  232725.000000  2.327250e+05  232725.000000   \nmean       41.127502       0.554364  2.351223e+05       0.570958   \nstd        18.189948       0.185608  1.189359e+05       0.263456   \nmin         0.000000       0.056900  1.538700e+04       0.000020   \n25%        29.000000       0.435000  1.828570e+05       0.385000   \n50%        43.000000       0.571000  2.204270e+05       0.605000   \n75%        55.000000       0.692000  2.657680e+05       0.787000   \nmax       100.000000       0.989000  5.552917e+06       0.999000   \n\n            loudness    speechiness  instrumentalness          tempo  \\\ncount  232725.000000  232725.000000     232725.000000  232725.000000   \nmean       -9.569885       0.120765          0.148301     117.666585   \nstd         5.998204       0.185518          0.302768      30.898907   \nmin       -52.457000       0.022200          0.000000      30.379000   \n25%       -11.771000       0.036700          0.000000      92.959000   \n50%        -7.762000       0.050100          0.000044     115.778000   \n75%        -5.501000       0.105000          0.035800     139.054000   \nmax         3.744000       0.967000          0.999000     242.903000   \n\n       time_signature  artist_name_mean_energy  \ncount   232725.000000            232725.000000  \nmean         0.971287                 0.570958  \nstd          0.115739                 0.222297  \nmin          0.000000                 0.000953  \n25%          1.000000                 0.436912  \n50%          1.000000                 0.613531  \n75%          1.000000                 0.734027  \nmax          1.250000                 0.999000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>popularity</th>\n      <th>danceability</th>\n      <th>duration_ms</th>\n      <th>energy</th>\n      <th>loudness</th>\n      <th>speechiness</th>\n      <th>instrumentalness</th>\n      <th>tempo</th>\n      <th>time_signature</th>\n      <th>artist_name_mean_energy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>232725.000000</td>\n      <td>232725.000000</td>\n      <td>2.327250e+05</td>\n      <td>232725.000000</td>\n      <td>232725.000000</td>\n      <td>232725.000000</td>\n      <td>232725.000000</td>\n      <td>232725.000000</td>\n      <td>232725.000000</td>\n      <td>232725.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>41.127502</td>\n      <td>0.554364</td>\n      <td>2.351223e+05</td>\n      <td>0.570958</td>\n      <td>-9.569885</td>\n      <td>0.120765</td>\n      <td>0.148301</td>\n      <td>117.666585</td>\n      <td>0.971287</td>\n      <td>0.570958</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>18.189948</td>\n      <td>0.185608</td>\n      <td>1.189359e+05</td>\n      <td>0.263456</td>\n      <td>5.998204</td>\n      <td>0.185518</td>\n      <td>0.302768</td>\n      <td>30.898907</td>\n      <td>0.115739</td>\n      <td>0.222297</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.056900</td>\n      <td>1.538700e+04</td>\n      <td>0.000020</td>\n      <td>-52.457000</td>\n      <td>0.022200</td>\n      <td>0.000000</td>\n      <td>30.379000</td>\n      <td>0.000000</td>\n      <td>0.000953</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>29.000000</td>\n      <td>0.435000</td>\n      <td>1.828570e+05</td>\n      <td>0.385000</td>\n      <td>-11.771000</td>\n      <td>0.036700</td>\n      <td>0.000000</td>\n      <td>92.959000</td>\n      <td>1.000000</td>\n      <td>0.436912</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>43.000000</td>\n      <td>0.571000</td>\n      <td>2.204270e+05</td>\n      <td>0.605000</td>\n      <td>-7.762000</td>\n      <td>0.050100</td>\n      <td>0.000044</td>\n      <td>115.778000</td>\n      <td>1.000000</td>\n      <td>0.613531</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>55.000000</td>\n      <td>0.692000</td>\n      <td>2.657680e+05</td>\n      <td>0.787000</td>\n      <td>-5.501000</td>\n      <td>0.105000</td>\n      <td>0.035800</td>\n      <td>139.054000</td>\n      <td>1.000000</td>\n      <td>0.734027</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>100.000000</td>\n      <td>0.989000</td>\n      <td>5.552917e+06</td>\n      <td>0.999000</td>\n      <td>3.744000</td>\n      <td>0.967000</td>\n      <td>0.999000</td>\n      <td>242.903000</td>\n      <td>1.250000</td>\n      <td>0.999000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y[:10])","execution_count":14,"outputs":[{"output_type":"stream","text":"[1. 1. 0. 0. 0. 0. 1. 0. 1. 1.]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Стандартная нормализация данных."},{"metadata":{"trusted":true},"cell_type":"code","source":"scale_features_std = StandardScaler()\nX = scale_features_std.fit_transform(X.to_numpy())","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Формирование обучающей и тестовой выборок."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nprint(len(train_X), \"train +\", len(val_X), \"test\")","execution_count":16,"outputs":[{"output_type":"stream","text":"174543 train + 58182 test\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Теперь, когда обучающая и тестовая выборки готовы, перейдем к основной части работы - реализации алгоритмов классификации."},{"metadata":{},"cell_type":"markdown","source":"# Импорт необходимых библиотек"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Логистическая регрессия"},{"metadata":{},"cell_type":"markdown","source":"**Теоретическая вставка.**"},{"metadata":{},"cell_type":"markdown","source":"**Логистическая регрессия** - это статистический метод бинарной классификации.\n\nМетод логистической регрессии основан на довольно сильных вероятностных предположениях, которые имеют сразу несколько интересных последствий:\n\n* линейный алгоритм классификации оказывается оптимальным байесовским классификатором;\n* однозначно определяется функция потерь;\n* возникает интересная дополнительная возможность наряду с классификацией объекта получать численные оценки вероятности его принадлежности каждому из классов.\n\n*Логистическая регрессия* применяется для прогнозирования вероятности возникновения некоторого события по значениям множества признаков. Для этого вводится зависимая переменная $y$, принимающая значения 0 и 1, и множество независимых переменных $x_1,\\ldots,x_n,$ на основе значений которых требуется вычислить вероятность принятия того или иного значения зависимой переменной.\n\n\nИтак, пусть объекты задаются $n$ числовыми признаками $$f_j : X \\rightarrow R,  j = 1\\ldots n$$ и пространство признаковых описаний $X = R^n.$ Пусть $Y$ - конечное множество меток классов, и задана обучающая выборка вида $X^m = \\left\\{(x_1,y_1), \\ldots ,(x_m, y_m)\\right\\}.$\n\nРассмотрим случай двух классов: $Y = {-1, +1}.$\nВ *логистической регрессии* строится линейный алгоритм классификации $a: X \\rightarrow Y$ вида\n\n$$\na(x, w) = sign\\left(\\sum_{j=1}^n w_j\\cdot f_j(x) - w_0\\right) = sign\\langle x, w\\rangle,\n$$\n\nгде $w_j$ − вес j-го признака, $w_0$ − порог принятия решения, $w = (w_0,\\ldots,w_n)$ − вектор весов, $\\langle x,w\\rangle$ − скалярное произведение признакового описания объекта на вектор весов. Предполагается, что искусственно введён нулевой признак: $f_0(x)= −1.$\n\nЗадача обучения линейного классификатора заключается в том, чтобы по выборке $X^m$ настроить вектор весов $w.$ В логистической регрессии для этого решается **задача минимизации эмпирического риска** с функцией потерь специального вида:\n$$\nQ(w)=\\sum_{i = 1}^m ln\\left(1 + exp(−y_i\\langle x_i,w \\rangle)\\right) \\rightarrow \\min_w,\n$$\n\nПосле того, как решение $w$ найдено, становится возможным не только вычислять классификацию $a(x) = sign \\langle x, w\\rangle$ для произвольного объекта $x,$ но и оценивать апостериорные вероятности его принадлежности классам:\n\n$$\nP\\langle y | x \\rangle = \\sigma(y\\langle x, w \\rangle), y \\in Y,\n$$\n\nгде $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ - **сигмоидная функция.**\n\nДля обучения модели можно использовать алгоритм **градиентного спуска** в силу дифференцируемости функционала ошибки."},{"metadata":{},"cell_type":"markdown","source":"# Реализация"},{"metadata":{},"cell_type":"markdown","source":"Определим необходимые **метрики качества.**\n\n*Accuracy* - доля правильных ответов алгоритма.\n\n$$ accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n\nЭта метрика редко используется, т.к. является бесполезной в задачах с неравными классами."},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(val_y, pred_y):\n    TP = (val_y * pred_y).sum()\n    TN = np.logical_not(bool(val_y.all()) | bool(pred_y.all())).sum()\n    return (TP + TN) / len(pred_y)","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Precision* - метрика, характеризующая точность определения классификатором принадлжености объектов к положительному классу (1).\n\n$$precision = \\frac{TP}{TP + FP}$$ "},{"metadata":{"trusted":true},"cell_type":"code","source":"def precision(val_y, pred_y):\n    TP = (val_y * pred_y).sum()\n    return TP / pred_y.sum()","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Recall* - метрика, характеризующая полноту определения классификатором принадлежности объектов к положительному классу (1).\n\n$$recall = \\frac{TP}{TP + FN}$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"def recall(val_y, pred_y):\n    TP = (val_y * pred_y).sum()\n    return TP / val_y.sum()","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*F-мера* - это среднее гармоническое precision и recall.\n\n$$F_\\beta = (1 + \\beta^2)\\cdot \\frac{precision\\cdot recall}{(\\beta^2\\cdot precision) + recall},$$\n\nгде $\\beta$ - вес точности в метрике.\n\nПри $\\beta = 1$ это среднее гармоническое. Будем использовать меру \n$$F_1 = \\frac{2\\cdot precision \\cdot recall}{precision + recall}$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"def F_1(val_y, pred_y):\n    pr = precision(val_y, pred_y)\n    rec = recall(val_y, pred_y)\n    return 2 * rec * pr / (pr + rec)","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Градиентный спуск.**\n\nЗадачу минимизации функции потерь будем решать с помощью алгоритма *градиентного спуска.* Его реализация представлена ниже."},{"metadata":{},"cell_type":"markdown","source":"Будем использовать следующие нормы.\n\n1. **Манхэттенское расстояние** - метрика $L_1,$ норма $l_1,$ которая для вектора представляет собой сумму модулей всех его элементов.\n\n$$\n||x||_1 = \\sum_i |x_i|\n$$\n\n2. **Евклидова норма** - метрика $L_2,$ норма $l_2,$ которая вляется геометрическим расстоянием между двумя точками в многомерном пространстве, вычисляемым по теореме Пифагора.\n\n$$\n||x||_2 = \\sqrt{\\sum_i |x_i|^2}\n$$\n\nРеализация регуляционных норм представлена ниже."},{"metadata":{"trusted":true},"cell_type":"code","source":"def L1(vector):\n    return np.abs(vector).sum()\ndef L2(vector):\n    return (vector**2).sum() #**0.5","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Соответсвующие нормам градиенты."},{"metadata":{"trusted":true},"cell_type":"code","source":"def L1_grad(vector):\n    return vector/ np.abs(vector)\ndef L2_grad(vector):\n    return vector","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Решатель на основе градиентного спуска."},{"metadata":{"trusted":true},"cell_type":"code","source":"class GradientDescent:\n    def __init__(self, speed, gradient_func, regulasator = None, C = 10.0, eps = 0.001, maxsteps = 250):\n        self.speed = speed\n        self.function = gradient_func\n        self.maxsteps = maxsteps\n        self.eps = eps\n        if regulasator == \"l1\":\n            self.regulasator = lambda w:  L1_grad(w) / C\n        elif regulasator == \"l2\":\n            self.regulasator = lambda w: L2_grad(w) / C\n        else:\n            self.regulasator = lambda w: 0.0\n    \n    def fit(self, train_X, train_y):\n        w_0 = np.zeros(train_X.shape[1])\n        w = np.random.random(train_X.shape[1])\n        k = 1\n        while np.linalg.norm(w - w_0) > self.eps and k <= self.maxsteps:\n            w_0 = w\n            temp = self.speed * ((1 / k)**0.5)\n            w = w - temp * (self.function(train_X, train_y, w) + self.regulasator(w))\n            k += 1        \n        return w","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Логистическая регрессия.**\n\n\n**Сигмоида:**\n\n$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n\n**Функция потерь:**\n\n$$ Loss(w)=\\sum_{i = 1}^m ln\\left(1 + e^{−y_i\\langle x_i,w \\rangle}\\right)$$\n\n**Градиент по вектору весов:**\n\n$$Grad\\_loss(w) = \\sum_{i = 1}^m - \\frac{y_i\\cdot x_i}{1 + e^{-y_i \\langle x_i, w \\rangle}}$$\n\nРеализация представлена ниже."},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return np.exp(-np.logaddexp(0, -x))\n\ndef logit_loss(wx, y):\n    return np.log(1 + np.exp(- wx * y)).sum()\n\ndef logit_grad(x, y, w):\n    k = y * sigmoid(-y * x.dot(w))\n    k = k.reshape((k.shape[0], 1))\n    return -(k * x).sum(axis = 0)","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Модель логистической регрессии на основе градиентного спуска представлена ниже."},{"metadata":{"trusted":true},"cell_type":"code","source":"class BinaryLogisticRegression:\n    def __init__(self, speed = 1.5, reg_type=None, C=2.0, eps=0.001, maxsteps=200):\n        self.solver = GradientDescent(speed, logit_grad, reg_type, C, eps, maxsteps)\n        self.w = None\n        \n    def fit(self, train_X, train_y):\n        y = np.array(train_y)\n        y[train_y == 0] = -1\n        x0 = np.ones((train_X.shape[0], 1))\n        X = np.hstack((x0, train_X))\n        self.w = self.solver.fit(X, y)\n        return self\n    \n    # returns predictes classes\n    def predict(self, val_X, border = 0):\n        x0 = np.ones((val_X.shape[0], 1))\n        X = np.hstack((x0, val_X))\n        Xw = X.dot(self.w)\n        pred_y = np.zeros(Xw.shape).astype(np.int8)\n        pred_y[Xw >= border] = 1\n        return pred_y\n    \n    def predict_proba(self, val_X):\n        x0 = np.ones((val_X.shape[0], 1))\n        X = np.hstack((x0, val_X))\n        Xw = X.dot(self.w)\n        return sigmoid(Xw)\n    \n    def score(self, val_X, val_y, metric=F_1):\n        return metric(val_y, self.predict(val_X))\n    \n    def weights(self):\n        return self.w\n    \n    def __repr__(self):\n        return \"Logistic Regression\"","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Обучим модель с L2 регулизацией, F_1 метрикой качества."},{"metadata":{"trusted":true},"cell_type":"code","source":"my_log_reg = BinaryLogisticRegression(reg_type='l2', C=1, maxsteps=1500, speed=0.02)\nmy_log_reg.fit(train_X, train_y)","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"Logistic Regression"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Logistic regression precision: \", my_log_reg.score(val_X, val_y))","execution_count":28,"outputs":[{"output_type":"stream","text":"Logistic regression precision:   0.6671368802398089\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Такой алгоритм логистической регрессии достигает точности до 70%.\nТеперь обучим модель из библиотеки sklearn."},{"metadata":{"trusted":true},"cell_type":"code","source":"skl_log_reg = LogisticRegression(penalty='l2', C=0.8, max_iter=250)\nskl_log_reg.fit(train_X, train_y)","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"LogisticRegression(C=0.8, max_iter=250)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Sklearn logistic regression precision: \", skl_log_reg.score(val_X, val_y))","execution_count":30,"outputs":[{"output_type":"stream","text":"Sklearn logistic regression precision:  0.7233508645285484\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Величина ошибки обеих моделей достаточно высока - это может быть связано с тем, что исходный целевой признак был грубо приведен к бинарному виду. Целевой признак в исходном наборе данных представляет собой некоторую величину на интервале от 0 до 1, а не две полярности - 0 или 1.\nТак же, самодельная реализация логистической регрессии уступает библиотечной версии. Это может быть связано с более оптимальным алгоритмом минимизации функции ошибки."},{"metadata":{},"cell_type":"markdown","source":"# KNN-метод"},{"metadata":{},"cell_type":"markdown","source":"Теоретическая вставка.\n\nДля произвольного объекта $u \\in X$ расположим элементы обучающей выборки $x_1,\\ldots,x_l$ в порядке возрастания расстояния до $u:$\n\n$$\n\\rho(u, x_u^{(1)}) \\leq \\rho(u, x_u^{(2)}) \\leq \\ldots \\leq \\rho(u, x_u^{(l)}),\n$$\n\nгде через $x_u^{(i)}$ обозначается $i-$й сосед объекта $u.$ Соответственно, ответ на $i-$м соседе есть $y_u^{(i)} = y^\\star(x_u^{(i)}).$ Таким образом, любой объект $u \\in X$ порождает свою перенумерацию этой выборки.\n\n*Метрический алгоритм классификации* с обучающей выборкой $X^l$ относит объект $u$ к тому классу $y \\in Y,$ для которого суммарный вес ближайших обучающих объектов $\\Gamma_y(u, X^l)$ максимален:\n\n$$\na(u; X^l) = \\arg \\max_{y \\in Y} \\Gamma_y(u, X^l); \\ \\ \\ \\Gamma_y(u, X^l) = \\sum_{i = 1}^l \\left[ y_u^{(i)} = y \\right] w(i, u);\n$$\n\nгде *весовая функция* $w(i, w)$ оценивает степень важности $i-$го соседа для классификации объекта $u.$ Функция $\\Gamma_y(u, X^l)$ называется *оценкой близости объекта* $u$ к классу $y.$\n\nОбучающая выборка $X_l$ играет роль параметра алгоритма $a.$ Настройка сводится к запоминанию выборки, и, возможно, оптимизации каких-то параметров весовой функции, однако сами объекты не подвергаются обработке и сохраняются «как есть». Алгоритм $a(u; X_l)$ строит локальную аппроксимацию выборки $X_l,$ причём вычисления откладываются до момента, пока не станет известен объект $u.$ По этой причине метрические алгоритмы относятся к методам ленивого обучения (lazy learning), в отличие от усердного обучения (eager learning), когда на этапе обучения строится функция, аппроксимирующая выборку.\n\nМетрические алгоритмы классификации относятся также к методам рассуждения по прецедентам (case-based reasoning, CBR). Здесь действительно можно говорить о «рассуждении», так как на вопрос «почему объект $u$ был отнесён к классу $y$?» алгоритм может дать понятное экспертам объяснение: «потому, что имеются схожие\nс ним прецеденты класса $y$», и предъявить список этих прецедентов.\n\n\n**KNN** или *метод k-ближайших соседей* - метрический алгоритм для автоматической классификации объектов или регрессии.\n\nВ случае использования метода для классификации объект присваивается тому классу, который является наиболее распространённым среди $k$ соседей данного элемента, классы которых уже известны. В случае использования метода для регрессии, объекту присваивается среднее значение по $k$ ближайшим к нему объектам, значения которых уже известны.\n\nАлгоритм может быть применим к выборкам с большим количеством атрибутов (многомерным). Для этого перед применением нужно определить *функцию расстояния*. Классический вариант такой функции - **евклидова метрика.**"},{"metadata":{},"cell_type":"markdown","source":"Крайние значения $k$ нежелательны. На практике оптимальное значение параметра $k$ определяют по критерию скользящего контроля с исключением объектов по одному (leave-one-out, LOO). Для каждого объекта $x_i \\in X^l$ проверяется, правильно ли он классифицируется по своим $k$ ближайшим соседям.\n\n$$\nLOO(k, X^l) = \\sum_{i=1}^l \\left[ a(x_i; X^l \\backslash \\{x_i\\}, k) \\neq y_i \\right] \\rightarrow min_k.\n$$\n\nЗаметим, что если классифицируемый объект $x_i$ не исключать из обучающей выборки, то ближайшим соседом $x_i$ всегда будет сам $x_i,$ и минимальное (нулевое) значение функционала $LOO(k)$ будет достигаться при $k = 1.$\n\nСуществует и *альтернативный вариант метода kNN:* в каждом классе выбирается $k-$ближайших к $u$ объектов, и объект $u$ относится к тому классу, для которого среднее расстояние до $k-$ближайших соседей минимально.\n\nНедостаток $kNN$ в том, что максимум может достигаться сразу на нескольких классах. В задачах с двумя классами\nэтого можно избежать, если взять нечётное $k.$ Более общая тактика, которая годится и для случая многих классов — ввести строго убывающую последовательность вещественных весов $w_i,$ задающих вклад $i-$го соседа в классификацию:\n\n$$\nw(i, u) = [i \\leq k] w_i; \\ \\ \\ \\ a(u; X^l, k) = \\arg \\max_{y \\in Y} \\sum_{i = 1}^k \\left[ y_u^{(i)} = y\\right] w_i.\n$$\n\n\nЕщё один способ задать веса соседям - определить $w_i$ как функцию от расстояния $\\rho(u, x_u^{(i)}),$ а не от ранга соседа $i.$ Введём *функцию ядра* $K(z),$ невозрастающую на $[0, \\infty).$ Положив $w(i, u) = K\\left(\\frac{1}{h} \\rho(u, x_u^{(i)})\\right)$ в общей формуле (3.1), получим алгоритм:\n\n$$\na(u; X^l, k) = \\arg \\max_{y \\in Y} \\sum_{i = 1}^k \\left[ y_u^{(i)} = y\\right] K\\left(\\frac{\\rho(u, x_u^{(i)})}{h}\\right).\n$$\n\nПараметр $h$ называется *шириной окна* и играет примерно ту же роль, что и число соседей $k.$\n\n**«Окно»** - это сферическая окрестность объекта $u$ радиуса $h,$ при попадании в которую обучающий объект $x_i$ «голосует» за отнесение объекта $u$ к классу $y_i.$\n\nМы пришли к этому алгоритму чисто эвристическим путём, однако он имеет более строгое обоснование в байесовской теории классификации, и, фактически, совпадает с *методом парзеновского окна.*\n\nПараметр $h$ можно задавать априори или определять по скользящему контролю. Зависимость $LOO(h)$, как правило, имеет характерный минимум, поскольку слишком узкие окна приводят к неустойчивой классификации; а слишком широкие - к вырождению алгоритма в константу.\n\nФиксация ширины окна $h$ не подходит для тех задач, в которых обучающие объекты существенно неравномерно распределены по пространству X. В окрестности одних объектов может оказываться очень много соседей, а в окрестности других - ни одного. В этих случаях применяется *окно переменной ширины.*\n\nВозьмём *финитное ядро* - невозрастающую функцию $K(z),$ положительную на отрезке $[0, 1],$ и равную нулю вне его. Определим $h$ как наибольшее число, при котором ровно $k$ ближайших соседей объекта $u$ получают ненулевые веса: $h(u) = \\rho(u, x_u^{(k+1)}).$ Тогда алгоритм принимает вид:\n\n$$\na(u; X^l, k) = \\arg \\max_{y \\in Y} \\sum_{i = 1}^k \\left[ y_u^{(i)} = y\\right] K\\left(\\frac{\\rho(u, x_u^{(i)})}{\\rho(u, x_u^{(k+1)})}\\right).\n$$\n\nЗаметим, что при финитном ядре классификация объекта сводится к поиску его соседей, тогда как при не финитном ядре (например, гауссовском) требуется перебор всей обучающей выборки."},{"metadata":{},"cell_type":"markdown","source":"# Реализация"},{"metadata":{},"cell_type":"markdown","source":"\n**Метрики.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def minkovski(x1, x2, p = 3):\n    return (np.abs(x1 - x2) ** p).T.sum(axis = 0)**(1.0 / p)\n\ndef evclid(x1, x2):\n    return minkovski(x1, x2, 2)","execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Возможные ядра:\n* биквадратное $$ K(z) = (1 - z^2)^2$$\n* триквадратное $$ K(z) = (1 - z^2)^3$$\n* трикубическое $$ K(z) = (1 - z^3)^3$$\n\nИх реализация представлена ниже."},{"metadata":{"trusted":true},"cell_type":"code","source":"def biquadratical_kernel(z):\n    return (1 - z**2)**2\n\ndef triquadratical_kernel(t):\n    return (1 - z**2)**3\n    \ndef triqubical_kernel(t):\n    return (1 - z**3)**3","execution_count":32,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Реализация бинарного $KNN-$классификатора с использованием евклидовой метрики."},{"metadata":{"trusted":true},"cell_type":"code","source":"class BinaryKNN:\n    def __init__(self, k, metric = evclid, parsen_kernel = None):\n        self.k = k\n        self.p = metric\n        self.X = None\n        self.Y = None\n        if parsen_kernel is None:\n            # all elements have equal weight\n            self.Kp = lambda t: 1.0\n        else:\n            self.Kp = parsen_kernel\n\n    def stolp_filtration(self):\n        pass\n    \n    def fit(self, train_X, train_y):\n        self.X = train_X\n        self.Y = train_y\n        # check correct of k\n        self.k = min(self.k, len(self.Y) - 1)\n        return self\n\n    def predict(self, val_X):\n        pred_y = np.zeros(len(val_X)).astype(np.int8)\n        for i in np.arange(len(val_X)):\n            pred_y[i] = self.predict_one(val_X[i])\n        return pred_y\n\n    def predict_one(self, x):\n        # compute all distances\n        r_x = self.p(self.X, x)\n        order = np.argsort(r_x)\n        h = r_x[order[self.k]]\n        order = order[:self.k]\n        Y_k = self.Y[order]\n        K = self.Kp(r_x[order] / h)\n        \n        pos_w = (K * Y_k).sum()\n        neg_w = (K * np.logical_not(Y_k)).sum()\n        \n        # class with more functional wins\n        return int(pos_w > neg_w) # 0 or 1\n\n    def score(self, val_X, val_y, metric=F_1):\n        return metric(val_y, self.predict(val_X))\n\n    def __repr__(self):\n        return \"KNN\"","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Т.к. обучение и тестирование модели заняло много времени, я решила сократить размеры тестировочной выборки.\n\nОбучение и тестирование $KNN-$модели представлено ниже."},{"metadata":{"trusted":true},"cell_type":"code","source":"my_knn = BinaryKNN(k=33)\nmy_knn.fit(train_X, train_y)","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"KNN"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"KNN-classificator precision: \", my_knn.score(val_X[0:10000], val_y[0:10000]))","execution_count":35,"outputs":[{"output_type":"stream","text":"KNN-classificator precision:  0.7251855287569574\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Точность работы KNN несколько выше точности, достигнутой логистической регрессией. Однако, здесь представлена работа алгоритма на шестой части обучающей выборки - KNN работает очень медленно. Возможно, из-за выбранного количества соседей или количества обучающих признаков в моей выборке."},{"metadata":{},"cell_type":"markdown","source":"Теперь обучим библиотечную *sklearn* классификатор и сравним полученные результаты."},{"metadata":{"trusted":true},"cell_type":"code","source":"skl_knn = KNeighborsClassifier(n_neighbors=33)\nskl_knn.fit(train_X, train_y)","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"KNeighborsClassifier(n_neighbors=33)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Sklearn KNN-classificator precision: \", skl_knn.score(val_X[0:10000], val_y[0:10000]))","execution_count":37,"outputs":[{"output_type":"stream","text":"Sklearn KNN-classificator precision:  0.763\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Библиотечная версия работает несколько лучше, но разница незначительна."},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree."},{"metadata":{},"cell_type":"markdown","source":"**Теоретическая вставка.**\n\n**Дерево решений** - это бинарное дерево, каждой вершине $v$ которого соответствуют подвыборки $R_v \\subset (X, Y).$ Для бинарной классификации примем $Y = \\{0, 1\\}.$\n\nВершины бывают двух видов:\n\n* *внутренние вершина* - это вершина, которые содержат предикаты вида $[f_j(x) \\lt t],$ результат которых делит $R_v$ на две подвыборки $R_l$ и $R_r,$ которые будут соответствовать левой вершине $(l)$ и правой вершине $(r)$ соответственно.\n* *лист* - это вершина, в которой записано предсказываемое значение целевой функции или результат работы алгоритма соответсвующего этой вершине поддерева: $c_v = \\underset{c}{\\operatorname{argmax}} \\sum\\limits^{|R_v|}_{i = 1} [y_i = c],$ где $(x_i, y_i) \\in R_v, c \\in Y.$\n\nЧтобы избежать переобучения дерева,используем алгоритом срижки деревьев Cost Complexity Pruning, который ищет $\\alpha-$оптимальное поддерево - то, у которого значение функционала $R_{\\alpha}(T) = R(T) + \\alpha |T|$ минимально."},{"metadata":{},"cell_type":"markdown","source":"# Реализация"},{"metadata":{},"cell_type":"markdown","source":"Реализация вершин дерева."},{"metadata":{"trusted":true},"cell_type":"code","source":"class BinaryNode:\n    def __init__(self, idxs=None, pos=None, neg=None, c=None):\n        self.predicat = None\n        self.left = None\n        self.right = None\n        self.positives = pos\n        self.negatives = neg\n        self.c = c\n        self.idxs = idxs\n\n    def set_left(self, left_node):\n        self.left = left_node\n\n    def set_idxs(self, idxs):\n        self.idxs = idxs\n\n    def set_right(self, right_node):\n        self.right = right_node\n\n    def set_predicat(self, predicat):\n        self.predicat = predicat\n\n    def set_class(self, c):\n        self.c = c\n        \n    def set_positives(self, positives):\n        self.positives = positives\n        \n    def set_negatives(self, negatives):\n        self.negatives = negatives\n\n    #getters:\n    def get_left(self):\n        return self.left\n\n    def get_right(self):\n        return self.right\n\n    def get_class(self):\n        return self.c\n\n    def get_idxs(self):\n        return self.idxs\n    \n    def get_positives(self):\n        return self.positives\n        \n    def get_negatives(self):\n        return self.negatives\n    \n    def get_len(self):\n        return self.idxs.shape[0]\n\n    #checkers:\n    def is_leaf(self):\n        return self.predicat is None\n\n    def is_inner(self):\n        return not self.is_leaf()\n    \n    def make_leaf(self):\n        self.predicat = None\n        self.left = None\n        self.right = None","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Критерии информативности."},{"metadata":{"trusted":true},"cell_type":"code","source":"def bingini(*args):\n    if len(args) == 2:\n        p = args[0] / (args[1]+args[0])\n    else:\n        p = args[0].sum() / args[0].shape[0]\n    return 2 * p * (1 - p)\n\ndef binentropy(*args):\n    if len(args) == 2:\n        p = args[0] / (args[1]+args[0])\n    else:\n        p = args[0].sum() / args[0].shape[0]\n    return -p*np.log(p) - (1 - p)*np.log(p)","execution_count":39,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Дерево решений."},{"metadata":{"trusted":true},"cell_type":"code","source":"class BinaryDescisionTree:\n    def __init__(self, criteria=bingini, pruning_cost=None, min_samples_split=2, random_sub_num=None):\n        self.CATEGORICAL_LEN = 10\n        self.is_categorical = None\n        self.categorical_vals = {}\n        self.H = criteria\n        self.root = None\n        self.min_split = min_samples_split\n        if random_sub_num is None:\n            self.random_subspace = False\n        else:\n            self.random_subspace = True\n            self.feature_num = random_sub_num\n        if pruning_cost is None:\n            self.pruning = False\n        else:\n            self.pruning = True\n            self.alpha = pruning_cost\n            \n    def node_classify(self, node):\n        positives = node.get_positives()\n        negatives = node.get_negatives()\n        if positives >= negatives:\n            node.set_class(1)\n        else:\n            node.set_class(0)\n            \n    # create root and recursive building tree\n    def build_tree(self):\n        self.root = BinaryNode(np.arange(self.Y.shape[0]))\n        self.root.set_positives(self.Y.sum())\n        negatives = self.root.get_len() - self.root.get_positives()\n        self.root.set_negatives(negatives)\n        self.recursive_creation(self.root)\n            \n    def stop_criteria(self, node):\n        if node.get_len() < self.min_split:\n            return True\n        positives = node.get_positives()\n        negatives = node.get_negatives()\n        return (negatives==0 or positives==0)\n    \n    def search_best_split(self, node):\n        X_iter = self.X[node.get_idxs()]\n        Y_iter = self.Y[node.get_idxs()]\n        positiv = node.get_positives()\n        negativ = node.get_negatives()\n        node_info = self.H(positiv, negativ)\n        best_gain = 0.0\n        best_j, best_t = 0, 0.0\n        # search in all features:\n        if self.random_subspace:\n            features = np.random.permutation(self.X.shape[1])\n            features = features[:self.feature_num]\n        else:\n            features = range(self.X.shape[1])\n\n        for j in features:\n            column = X_iter[:, j]\n            if self.is_categorical[j]:\n                possible_vals = self.categorical_vals[j]\n                for i in range(1, possible_vals.shape[0]):\n                    mask = column < possible_vals[i]\n                    Y_r = Y_iter[mask]\n                    if Y_r.shape[0] == 0 or Y_r.shape[0] == Y_iter.shape[0]:\n                        continue\n                    right_pos = Y_r.sum()\n                    right_neg = Y_r.shape[0] - right_pos\n                    right_gini = self.H(right_pos, right_neg)\n                    left_gini = self.H(positiv - right_pos, negativ - right_neg)\n                    gain = node_info\n                    gain -= (Y_r.shape[0]*right_gini/node.get_len())\n                    gain -= (1 - Y_r.shape[0]/node.get_len())*left_gini\n                    if gain > best_gain:\n                        best_t = possible_vals[i]\n                        best_j = j\n                        best_gain = gain  \n                continue\n            # else standart search:\n            sorted_col = np.argsort(column)\n            right_neg = 0\n            right_pos = 0\n            last_t = column[sorted_col[0]]\n            for i in range(1, column.shape[0]):\n                if Y_iter[sorted_col[i-1]]:\n                    right_pos += 1\n                else:\n                    right_neg += 1\n                    \n                idx = sorted_col[i]\n                if column[idx] == last_t:\n                    continue\n                \n                last_t = column[idx]\n                right_gini = self.H(right_pos, right_neg)\n                left_gini = self.H(positiv - right_pos, negativ - right_neg)\n                gain = node_info\n                gain -= (i*right_gini/node.get_len()) + (1 - i/node.get_len())*left_gini\n                # needs best gain split\n                if gain > best_gain:\n                    best_t = column[idx]\n                    best_j = j\n                    best_gain = gain\n                    \n        if best_gain > 0.0:\n            return best_j, best_t\n    \n    # create 2 new nodes: left and right\n    def split_node(self, node, j, t):\n        predicat = lambda x: x[j] < t\n        node.set_predicat(predicat)\n        column = self.X[node.get_idxs(), j]\n        mask = column < t\n        right_idxs = node.get_idxs()[mask]\n        left_idxs = node.get_idxs()[np.logical_not(mask)]\n        right_pos = self.Y[right_idxs].sum()\n        left_pos = self.Y[left_idxs].sum()\n        right_neg = right_idxs.shape[0] - right_pos\n        left_neg = left_idxs.shape[0] - left_pos\n        # create nodes\n        node.set_left(BinaryNode(left_idxs, left_pos, left_neg))\n        node.set_right(BinaryNode(right_idxs, right_pos, right_neg))\n    \n    # recursive function for nodes:\n    def recursive_creation(self, node):\n        self.node_classify(node)\n        if self.stop_criteria(node):\n            return\n        jt = self.search_best_split(node)\n        if jt is None:\n            return\n\n        # split node for 2 child:\n        self.split_node(node, *jt)\n        self.recursive_creation(node.get_left())\n        self.recursive_creation(node.get_right())\n    \n    def tree_pruning(self, node):\n        if node.get_class():\n            R_a = node.get_negatives() / node.get_len()\n        else:\n            R_a = node.get_positives() / node.get_len()\n        R_a += self.alpha\n        if node.is_leaf():\n            return R_a\n        R_al = self.tree_pruning(node.get_left())\n        R_ar = self.tree_pruning(node.get_right())\n        if R_a <= R_al + R_ar:\n            node.make_leaf()\n            return R_a\n        return R_al + R_ar\n    \n    def search_categorical(self):\n        self.is_categorical = np.zeros(self.X.shape[1]).astype(np.int8)\n        for j in range(self.X.shape[1]):\n            uniq = np.unique(self.X[:, j])\n            if uniq.shape[0] < self.CATEGORICAL_LEN:\n                self.categorical_vals[j] = uniq\n                self.is_categorical[j] = 1\n            \n\n    def fit(self, train_X, train_y):\n        self.X = train_X\n        self.Y = train_y\n        self.idxs = np.arange(train_X.shape[0])\n        self.search_categorical() \n        self.build_tree()\n        # pruning tree\n        if self.pruning:\n            self.tree_pruning(self.root)\n\n        del self.X\n        del self.Y\n        del self.idxs\n        return self\n\n    def predict(self, val_X):\n        pred_y = np.zeros(val_X.shape[0]).astype(np.int8)\n        # for each elem in X predict result:\n        for i in np.arange(val_X.shape[0]):\n            pred_y[i] = self.predict_one(val_X[i])\n        return pred_y\n\n    def predict_one(self, x):\n        node = self.root\n        while node.is_inner():\n            if node.predicat(x):\n                node = node.get_right()\n            else:\n                node = node.get_left()\n        # if in leaf:\n        return node.get_class()\n\n    def score(self, val_X, val_y, metric=F_1):\n        return metric(val_y, self.predict(val_X))\n\n    def __repr__(self):\n        return \"Decision Tree\"","execution_count":40,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Результат работы моделей представлен ниже."},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = 0.00001","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_desc_tree = BinaryDescisionTree(pruning_cost=alpha)\nmy_desc_tree.fit(train_X, train_y)","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"Decision Tree"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Decision Tree precision: \", my_desc_tree.score(val_X, val_y))","execution_count":43,"outputs":[{"output_type":"stream","text":"Decision Tree precision:  0.7696221652455628\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Для библиотечной реализации коэффициент $\\alpha$ для \"стрижки дерева\" подобран немного больше."},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = 0.000008","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skl_desc_tree = DecisionTreeClassifier(ccp_alpha=alpha)\nskl_desc_tree.fit(train_X, train_y)","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"DecisionTreeClassifier(ccp_alpha=8e-06)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Sklearn Decision Tree precision: \", skl_desc_tree.score(val_X, val_y))","execution_count":46,"outputs":[{"output_type":"stream","text":"Sklearn Decision Tree precision:  0.8023099927812726\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Библиотечная версия дерева решений работает несколько точнее, однако эта разница незначительна."},{"metadata":{},"cell_type":"markdown","source":"# Random Forest."},{"metadata":{},"cell_type":"markdown","source":"**Теоретическая справка.**\n\n**Random forest** - алгоритм машинного обучения, предложенный Лео Брейманом и Адель Катлер, заключающийся в использовании ансамбля решающих деревьев. Алгоритм сочетает в себе две основные идеи: *метод бэггинга Бреймана* и *метод случайных подпространств*. \n\nАлгоритм применяется для задач классификации, регрессии и кластеризации. Основная идея заключается в использовании большого ансамбля решающих деревьев, каждое из которых само по себе даёт очень невысокое качество классификации, но за счёт их большого количества результат получается хорошим.\n\n**Алгоритм обучения классификатора.**\n\nПусть обучающая выборка состоит из $N$ образцов, размерность пространства признаков равна $M$, и задан параметр $m$ (в задачах классификации обычно $m \\approx \\sqrt M$) как неполное количество признаков для обучения.\n\nБудем строить случайный лес с помощью *бэггинга*:\n\n* сгенерируем случайную подвыборку **с повторениями** размером N из обучающей выборки. Таким образом, некоторые образцы попадут в неё два или более раза, а в среднем $N(1 - \\frac{1}{N})^N$ (при больших $N$ примерно $\\frac{N}{\\varepsilon}$, где $\\varepsilon$ — основание натурального логарифма) образцов не войдут в неё вообще. Те образцы, которые не попали в выборку, называются **out-of-bag (неотобранные)**.\n* построим решающее дерево, классифицирующее образцы данной подвыборки, причём в ходе создания очередного узла дерева будем выбирать набор признаков, на основе которых производится разбиение (не из всех $M$ признаков, а лишь из $m$ случайно выбранных). Выбор наилучшего из этих $m$ признаков может осуществляться различными способами. В оригинальном коде Бреймана используется критерий Джини, применяющийся также в алгоритме построения решающих деревьев CART. В некоторых реализациях алгоритма вместо него используется критерий прироста информации.\n* дерево строится до полного исчерпания подвыборки и не подвергается процедуре прунинга.\n\nКлассификация объектов проводится путём голосования: каждое дерево ансамбля относит классифицируемый объект к одному из классов, и побеждает класс, за который проголосовало наибольшее число деревьев.\n\nОптимальное число деревьев подбирается таким образом, чтобы минимизировать ошибку классификатора на тестовой выборке. В случае её отсутствия, минимизируется оценка ошибки *out-of-bag*: тех образцов, которые не попали в обучающую подвыборку за счёт повторений (их примерно $\\frac{N}{\\varepsilon}$ ).\n"},{"metadata":{},"cell_type":"markdown","source":"# Реализация."},{"metadata":{"trusted":true},"cell_type":"code","source":"class BinaryRandomForest:\n    def __init__(self, Ntrees=20, criteria=bingini, random_sub_num=None):\n        self.N = Ntrees\n        self.criteria = criteria\n        self.sub_space_num = random_sub_num\n        # list for trained models\n        self.trees = [None for _ in range(self.N)]\n\n    # get random sample for tree training\n    def bootstrap_sample(self, X, Y):\n        # indexes of X and Y\n        indexes = np.arange(len(Y))\n        # rundom indexes with repeats\n        indexes = np.random.choice(indexes, len(indexes))\n        # bootstrap sample\n        return X[indexes], Y[indexes]\n    \n    def fit(self, train_X, train_y):\n        if self.sub_space_num is None:\n            self.sub_space_num = int(len(train_X)**(1/2))\n        # train N trees with \n        for i in range(self.N):\n            # create tree(use our class of tree)\n            self.trees[i] = BinaryDescisionTree(\n                self.criteria, # user criteria\n                None, # trees without prunning\n                2, # build while it possible \n                self.sub_space_num) # num of random features in random space method\n            # and train tree:\n            self.trees[i].fit(*self.bootstrap_sample(train_X, train_y))\n        return self\n            \n\n    def predict(self, val_X):\n        voices = np.zeros(len(val_X))\n        # make vote:\n        for tree in self.trees:\n            voices += tree.predict(val_X)\n        # compute winners:\n        pred_y = voices >= (self.N + 1) // 2\n        return pred_y.astype(np.int8)\n\n    \n    def score(self, val_X, val_y, metric=F_1):\n        return metric(val_y, self.predict(val_X))\n\n    def __repr__(self):\n        return \"Random Forest\"","execution_count":47,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Обучение и тестирование моделей представлено ниже.\nОбучение длилось очень долго, и я решила сократить обучающую выборку."},{"metadata":{"trusted":true},"cell_type":"code","source":"size = int(np.size(train_X)/20)","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_rand_for = BinaryRandomForest()\nmy_rand_for.fit(train_X[:size], train_y[:size])","execution_count":49,"outputs":[{"output_type":"execute_result","execution_count":49,"data":{"text/plain":"Random Forest"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Random Forest precision: \", my_rand_for.score(val_X, val_y))","execution_count":50,"outputs":[{"output_type":"stream","text":"Random Forest precision:  0.7896114897385546\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"skl_rand_for = RandomForestClassifier(n_estimators=20)\nskl_rand_for.fit(train_X, train_y)","execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"RandomForestClassifier(n_estimators=20)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Sklearn Random Forest precision: \", skl_rand_for.score(val_X, val_y))","execution_count":52,"outputs":[{"output_type":"stream","text":"Sklearn Random Forest precision:  0.8501598432504899\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Величина ошибки обучения моделей сильно отличается, однако это может быть вызвано сокращением выборки - в любом случае, моя модель очень долго обучается."},{"metadata":{},"cell_type":"markdown","source":"# Выводы."},{"metadata":{},"cell_type":"markdown","source":"Выполнив данную лабораторную работу, я познакомилась \"изнутри\" с разными классификаторами, которые уже успешно реализованы в библиотеке Scikit-learn. Каждый алгоритм показал примерно одинаковую точность на моем наборе данных - к слову, в этой лабораторной работе я столкнулась с \"несовершенствами\" выбранного датасета, а именно - мне пришлось привести изначально небинарный целевой признак к нужному виду, и эта \"оценка\" отразилась на зависимостях между целевым признаком и обучающими признаками. Самыми медленными алгоритмами оказались KNN и RandomForest, в силу большого числа признаков и числа соседей (числа решающих деревьев). Точность моей и библиотечной реализаций отличается не сильно, но при этом не превышает 80%.\nВозможно, при решении задачи небинарной классификации, результаты оказались бы выше в силу изначального датасета - в первой лабораторной работе точность библиотечного алгоритма \"случайного леса\" достигала до 90%."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
